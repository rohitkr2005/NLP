{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a48e70",
   "metadata": {},
   "source": [
    "# Q1\n",
    "1.\tTokenization: \n",
    "a.\tWrite program using nltk.word_tokenize, TreebankWordTokenizer, wordpunct_tokenize, and TweetTokenizer on 300 sentences from Gutenberg (e.g., Austen) and 300 tweets.\n",
    "b.\tReport token counts, handling of emojis/hashtags/URLs, and sample tokenization differences.\n",
    "c.\thttps://www.nltk.org/book/ch02.html\n",
    "d.\thttps://www.nltk.org/howto/corpus.html\n",
    "\n",
    "lookup error solved (internet connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94385ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\BALRAM\n",
      "[nltk_data]     MANDAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\BALRAM\n",
      "[nltk_data]     MANDAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\BALRAM\n",
      "[nltk_data]     MANDAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Data loaded.\n",
      "\n",
      "--- Token Counts ---\n",
      "Gutenberg (word_tokenize): 10135 tokens\n",
      "Gutenberg (TreebankWordTokenizer): 10135 tokens\n",
      "Gutenberg (wordpunct_tokenize): 10059 tokens\n",
      "Tweets (TweetTokenizer): 4004 tokens\n",
      "\n",
      "--- Sample Tokenization Differences ---\n",
      "\n",
      "Original Gutenberg Sample: 'It isn't 'proper', is it?'\n",
      "  word_tokenize: ['It', 'is', \"n't\", \"'proper\", \"'\", ',', 'is', 'it', '?']\n",
      "  Treebank:      ['It', 'is', \"n't\", \"'proper\", \"'\", ',', 'is', 'it', '?']\n",
      "  wordpunct:     ['It', 'isn', \"'\", 't', \"'\", 'proper', \"',\", 'is', 'it', '?']\n",
      "\n",
      "Original Tweet Sample: 'This is so cool! :D #NLP @gemini http://ai.google'\n",
      "  word_tokenize:  ['This', 'is', 'so', 'cool', '!', ':', 'D', '#', 'NLP', '@', 'gemini', 'http', ':', '//ai.google']\n",
      "  TweetTokenizer: ['This', 'is', 'so', 'cool', '!', ':D', '#NLP', '@gemini', 'http://ai.google']\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, twitter_samples\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer\n",
    "\n",
    "# Download Data\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Data\n",
    "print(\"\\nLoading data...\")\n",
    "gutenberg_sents = gutenberg.sents('austen-persuasion.txt')[:300]\n",
    "gutenberg_text = \" \".join([\" \".join(s) for s in gutenberg_sents])\n",
    "tweets = twitter_samples.strings('positive_tweets.json')[:300]\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# Instantiate Tokenizers\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "#Process Data and Report Counts\n",
    "print(\"\\n--- Token Counts ---\")\n",
    "\n",
    "# a) word_tokenize (standard)\n",
    "tokens_word_tokenize = word_tokenize(gutenberg_text)\n",
    "print(f\"Gutenberg (word_tokenize): {len(tokens_word_tokenize)} tokens\")\n",
    "\n",
    "# b) TreebankWordTokenizer\n",
    "tokens_treebank = treebank_tokenizer.tokenize(gutenberg_text)\n",
    "print(f\"Gutenberg (TreebankWordTokenizer): {len(tokens_treebank)} tokens\")\n",
    "\n",
    "# c) wordpunct_tokenize\n",
    "tokens_wordpunct = wordpunct_tokenize(gutenberg_text)\n",
    "print(f\"Gutenberg (wordpunct_tokenize): {len(tokens_wordpunct)} tokens\")\n",
    "\n",
    "# d) TweetTokenizer\n",
    "# We tokenize each tweet, then flatten the list of lists\n",
    "tokens_tweets_nested = [tweet_tokenizer.tokenize(t) for t in tweets]\n",
    "tokens_tweets_flat = [token for sublist in tokens_tweets_nested for token in sublist]\n",
    "print(f\"Tweets (TweetTokenizer): {len(tokens_tweets_flat)} tokens\")\n",
    "\n",
    "\n",
    "# Report Sample Differences\n",
    "print(\"\\n--- Sample Tokenization Differences ---\")\n",
    "\n",
    "# Sample for Gutenberg\n",
    "sample_gutenberg = \"It isn't 'proper', is it?\"\n",
    "print(f\"\\nOriginal Gutenberg Sample: '{sample_gutenberg}'\")\n",
    "print(f\"  word_tokenize: {word_tokenize(sample_gutenberg)}\")\n",
    "print(f\"  Treebank:      {treebank_tokenizer.tokenize(sample_gutenberg)}\")\n",
    "print(f\"  wordpunct:     {wordpunct_tokenize(sample_gutenberg)}\")\n",
    "\n",
    "# Sample for Twitter\n",
    "sample_tweet = \"This is so cool! :D #NLP @gemini http://ai.google\"\n",
    "print(f\"\\nOriginal Tweet Sample: '{sample_tweet}'\")\n",
    "# We use word_tokenize on the tweet for comparison\n",
    "print(f\"  word_tokenize:  {word_tokenize(sample_tweet)}\")\n",
    "print(f\"  TweetTokenizer: {tweet_tokenizer.tokenize(sample_tweet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbfd83",
   "metadata": {},
   "source": [
    "# Q2\n",
    "2.\tStopword: \n",
    "a.\tStart with NLTK stopwords on the Inaugural or Reuters corpus, then build a domain-specific stoplist (e.g., “said”, “u.s.”, “inc”).  \n",
    "b.\tShow top-20 frequent tokens before/after with nltk.FreqDist and discuss changes.\n",
    "c.\thttps://www.nltk.org/book/ch02.html\n",
    "d.\thttps://www.nltk.org/api/nltk.probability.FreqDist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5934e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to C:\\Users\\BALRAM\n",
      "[nltk_data]     MANDAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\BALRAM\n",
      "[nltk_data]     MANDAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Total tokens in Inaugural corpus: 156288\n",
      "\n",
      "--- Top 20 Tokens (Before Removal) ---\n",
      "[('the', 10321), (',', 7499), ('of', 7276), ('and', 5553), ('.', 5186), ('to', 4688), ('in', 2858), ('a', 2344), ('our', 2295), ('we', 1924), ('that', 1859), ('be', 1545), ('is', 1519), ('it', 1427), ('for', 1247), ('by', 1104), ('have', 1050), ('will', 1037), ('which', 1009), ('not', 996)]\n",
      "\n",
      "--- Top 20 Tokens (After Removal) ---\n",
      "Total tokens after filtering: 65679\n",
      "[('government', 611), ('people', 602), ('us', 505), ('upon', 371), ('great', 356), ('world', 356), ('nation', 348), ('states', 344), ('country', 336), ('every', 310), ('one', 278), ('new', 262), ('peace', 260), ('america', 258), ('citizens', 256), ('power', 246), ('public', 229), ('time', 226), ('united', 212), ('constitution', 210)]\n",
      "\n",
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import inaugural, stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Download Data\n",
    "nltk.download('inaugural')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load Data\n",
    "print(\"\\nLoading data...\")\n",
    "raw_tokens = inaugural.words()\n",
    "print(f\"Total tokens in Inaugural corpus: {len(raw_tokens)}\")\n",
    "\n",
    "# Process (Before Stopword Removal)\n",
    "print(\"\\n--- Top 20 Tokens (Before Removal) ---\")\n",
    "\n",
    "raw_tokens_lower = [t.lower() for t in raw_tokens]\n",
    "fdist_before = FreqDist(raw_tokens_lower)\n",
    "print(fdist_before.most_common(20))\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "stopword_custom = {\n",
    "    'mr', 'mrs', 'u', 's', 't', # common short forms\n",
    "    'must', 'would', 'shall', 'may' # common modals\n",
    "}\n",
    "\n",
    "# Combine the lists\n",
    "final_stopword = stopword.union(stopword_custom)\n",
    "\n",
    "# Process (After Stopword Removal)\n",
    "print(\"\\n--- Top 20 Tokens (After Removal) ---\")\n",
    "\n",
    "# We filter out stopwords AND non-alphabetic tokens (punctuation)\n",
    "filtered_tokens = []\n",
    "for token in raw_tokens_lower:\n",
    "    if token.isalpha() and token not in final_stopword:\n",
    "        filtered_tokens.append(token)\n",
    "\n",
    "# Generate new FreqDist\n",
    "fdist_after = FreqDist(filtered_tokens)\n",
    "print(f\"Total tokens after filtering: {len(filtered_tokens)}\")\n",
    "print(fdist_after.most_common(20))\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098bfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
